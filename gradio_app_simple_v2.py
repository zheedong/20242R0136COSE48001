import os
from io import BytesIO
import random

import cv2
import gradio as gr
import numpy as np
import torch
from PIL import Image
from diffusers import DDIMScheduler, AutoencoderKL, ControlNetModel, StableDiffusionControlNetPipeline
from insightface.app import FaceAnalysis
from insightface.utils import face_align

from uniportrait import inversion
from uniportrait.uniportrait_attention_processor import attn_args
from uniportrait.uniportrait_pipeline import UniPortraitPipeline
from controlnet_aux import OpenposeDetector

port = 7860

device = "cuda"
torch_dtype = torch.float16

# Base model paths
base_model_path = "SG161222/Realistic_Vision_V5.1_noVAE"
vae_model_path = "stabilityai/sd-vae-ft-mse"
controlnet_pose_ckpt = "lllyasviel/control_v11p_sd15_openpose"

# Specific model paths
image_encoder_path = "models/IP-Adapter/models/image_encoder"
ip_ckpt = "models/IP-Adapter/models/ip-adapter_sd15.bin"
face_backbone_ckpt = "models/glint360k_curricular_face_r101_backbone.bin"
uniportrait_faceid_ckpt = "models/uniportrait-faceid_sd15.bin"
uniportrait_router_ckpt = "models/uniportrait-router_sd15.bin"

# Load ControlNet
pose_controlnet = ControlNetModel.from_pretrained(controlnet_pose_ckpt, torch_dtype=torch_dtype)
pose_processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')

# Load Stable Diffusion pipeline
noise_scheduler = DDIMScheduler(
    num_train_timesteps=1000,
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear",
    clip_sample=False,
    set_alpha_to_one=False,
    steps_offset=1,
)
vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch_dtype)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    base_model_path,
    controlnet=[pose_controlnet],
    torch_dtype=torch_dtype,
    scheduler=noise_scheduler,
    vae=vae,
)

# Load UniPortrait pipeline
uniportrait_pipeline = UniPortraitPipeline(
    pipe,
    image_encoder_path,
    ip_ckpt=ip_ckpt,
    face_backbone_ckpt=face_backbone_ckpt,
    uniportrait_faceid_ckpt=uniportrait_faceid_ckpt,
    uniportrait_router_ckpt=uniportrait_router_ckpt,
    device=device,
    torch_dtype=torch_dtype,
)

# Load face detection assets
face_app = FaceAnalysis(providers=['CUDAExecutionProvider'], allowed_modules=["detection"])
face_app.prepare(ctx_id=0, det_size=(640, 640))

def pad_np_bgr_image(np_image, scale=1.25):
    assert scale >= 1.0, "scale should be >= 1.0"
    pad_scale = scale - 1.0
    h, w = np_image.shape[:2]
    top = bottom = int(h * pad_scale)
    left = right = int(w * pad_scale)
    ret = cv2.copyMakeBorder(np_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128, 128, 128))
    return ret, (left, top)

def process_faceid_image(pil_faceid_image):
    np_faceid_image = np.array(pil_faceid_image.convert("RGB"))
    img = cv2.cvtColor(np_faceid_image, cv2.COLOR_RGB2BGR)
    faces = face_app.get(img)  # BGR image
    if len(faces) == 0:
        # Padding and try again
        _h, _w = img.shape[:2]
        _img, left_top_coord = pad_np_bgr_image(img)
        faces = face_app.get(_img)
        if len(faces) == 0:
            print("Warning: No face detected in the image. Continue processing...")
            return None

        min_coord = np.array([0, 0])
        max_coord = np.array([_w, _h])
        sub_coord = np.array([left_top_coord[0], left_top_coord[1]])
        for face in faces:
            face.bbox = np.minimum(
                np.maximum(face.bbox.reshape(-1, 2) - sub_coord, min_coord), max_coord
            ).reshape(4)
            face.kps = face.kps - sub_coord

    faces = sorted(
        faces,
        key=lambda x: abs((x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1])),
        reverse=True,
    )
    if len(faces) == 0:
        print("No faces found in the image.")
        return None
    faceid_face = faces[0]
    norm_face = face_align.norm_crop(img, landmark=faceid_face.kps, image_size=224)
    pil_faceid_align_image = Image.fromarray(
        cv2.cvtColor(norm_face, cv2.COLOR_BGR2RGB)
    )
    return pil_faceid_align_image

def prepare_single_faceid_cond_kwargs(pil_faceid_image=None):
    pil_faceid_align_images = []
    if pil_faceid_image:
        processed_image = process_faceid_image(pil_faceid_image)
        if processed_image is not None:
            pil_faceid_align_images.append(processed_image)

    single_faceid_cond_kwargs = None
    if len(pil_faceid_align_images) > 0:
        single_faceid_cond_kwargs = {"refs": pil_faceid_align_images}

    return single_faceid_cond_kwargs

def get_n_random_pose_images(folder_path: str, n: int = 4):
    """í´ë” ë‚´ ì—¬ëŸ¬ ì´ë¯¸ì§€(.png, .jpg, .jpeg, .webp) ì¤‘ nê°œë¥¼ ì¤‘ë³µ ì—†ì´ ëœë¤ ì„ íƒ."""
    valid_ext = {'.png', '.jpg', '.jpeg', '.webp'}
    if not os.path.exists(folder_path):
        return []

    files = [
        os.path.join(folder_path, f) for f in os.listdir(folder_path)
        if os.path.splitext(f.lower())[-1] in valid_ext
    ]
    if len(files) < n:
        print(f"Warning: '{folder_path}'ì— ì´ë¯¸ì§€ê°€ {len(files)}ì¥ë°–ì— ì—†ì–´ {n}ê°œ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # í•„ìš”í•˜ë‹¤ë©´ random.sample() í˜¸ì¶œ ì‹œ ì—ëŸ¬ê°€ ë‚©ë‹ˆë‹¤. 
        # ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ files ì „ì²´ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜, ì¤‘ë³µ í—ˆìš©(random.choices)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.
        # ì—¬ê¸°ì„œëŠ” ì¤‘ë³µ í—ˆìš© ë²„ì „ìœ¼ë¡œ í• ê²Œìš”.
        return random.choices(files, k=n)  # ì¤‘ë³µ í—ˆìš©
        
    return random.sample(files, k=n)

def text_to_multi_id_generation_process(pil_faceid_images):
    num_persons = len(pil_faceid_images)

    if num_persons == 2:
        num_persons_word = "two"
        negative_prompt = "nsfw, extra hands, extra arms, one person, three people, out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature,",
        negative_prompts = negative_prompt * 4
        pose_folder = "assets/2_people"

    elif num_persons == 3:
        num_persons_word = "three"
        negative_prompt = "nsfw, extra hands, extra arms, one person, two people, out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature,",
        negative_prompts = negative_prompt * 4
        pose_folder = "assets/3_people"

    elif num_persons == 4:
        num_persons_word = "four"
        negative_prompt = "nsfw, extra hands, extra arms, one person, two people, three people, out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature,",
        negative_prompts = negative_prompt * 4
        pose_folder = "assets/4_people"

    else:
        num_persons_word = ""
        negative_prompt = "nsfw, extra hands, extra arms, out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature,",
        negative_prompts = negative_prompt * 4
        pose_folder = None  # ControlNet ë¯¸ì ìš©

    # prompts/negative_promptsëŠ” 4ê°œì”© ìˆë‹¤ê³  ê°€ì •
    # (ê°ê°ì˜ promptì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ poseë¥¼ ë§¤ì¹­)
    prompts = [
        f"A DSLR photo of moody and atmospheric film noir scene featuring {num_persons_word} people with intense and brooding expressions, captured in dramatic lighting and shadowy contrasts for a sense of mystery and intrigue. High qaulity, Greyscale, Cinematic",
        f"A vivid and graphic comic book-style illustration of {num_persons_word} people with animated and exaggerated expressions, featuring larger-than-life contours and dynamic action that convey a sense of lightheartedness and fun.",
        f"A photo of {num_persons_word} people in a family",
        f"A photo of {num_persons_word} people, best quality, realistic details, vivid colors, natural lighting",
    ]

    # ì´ì œ ê° í”„ë¡¬í”„íŠ¸ë§ˆë‹¤ 1ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë¯€ë¡œ, ì´ 4Ã—1=4ì¥
    # í•˜ì§€ë§Œ "í”„ë¡¬í”„íŠ¸ x í¬ì¦ˆ"ë¥¼ 4Ã—4=16ì¥ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ 
    # -> ê° promptë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ pose ì´ë¯¸ì§€ë¥¼ ë§¤ì¹­í•´ì•¼ í•¨.
    # pose_image_list = get_n_random_pose_images(pose_folder, n=4)
    # => ì´ 4ê°œì˜ pose ì´ë¯¸ì§€ì™€ 4ê°œì˜ promptë¥¼ 1:1 ë§¤ì¹­ (prompt i -> pose_image_list[i])
    # => num_samples_per_prompt = 1
    # => ìµœì¢… 4 prompts Ã— 4 pose = 4Ã—1 = 4ì¥ì´ ì•„ë‹ˆë¼, ê° promptì— ëŒ€í•´ 1ì¥ì”©ë§Œ ìƒì„±í•˜ë¯€ë¡œ 4ì¥ ì´í•©
    # 
    # ë§Œì•½ "ê° prompt + ê° pose"ë¡œ ì¡°í•©(4promptÃ—4pose=16ì¥)ì„ ë§Œë“¤ë ¤ë©´ 
    # ì´ì¤‘ë£¨í”„ë‚˜ ë‹¤ë¥¸ ë¡œì§ì„ ì¨ì•¼ í•¨.
    
    # ì—¬ê¸°ì„œëŠ” "ê° promptì— ëŒ€í•´ í•˜ë‚˜ì”©, ê·¸ poseë„ ê°ì ë‹¬ë¼ì„œ 4ì¥"ì´ ì•„ë‹ˆë¼
    # "prompt 4ê°œ Ã— pose 4ê°œ â†’ ì´ 16ì¥"ì„ ë§Œë“¤ë ¤ë©´:
    # 1) pose ì´ë¯¸ì§€ë¥¼ 4ê°œ ë½‘ì•„ ë†“ìŒ
    # 2) ë°”ê¹¥ ë£¨í”„ëŠ” pose 4ê°œ, ì•ˆìª½ ë£¨í”„ëŠ” prompt 4ê°œ
    #    or prompt 4ê°œë¥¼ ëŒë©´ì„œ, pose 4ê°œë¥¼ ê°ê° ì¨ì„œ 4Ã—4=16ì¥ 
    
    # ì•„ë˜ ì˜ˆì‹œëŠ” "ê° poseë§ˆë‹¤ ëª¨ë“  promptë¥¼ ë°˜ë³µ" => 16ì¥
    pose_image_list = []
    if pose_folder:
        pose_image_list = get_n_random_pose_images(pose_folder, n=4)
    else:
        # ControlNet ë¯¸ì ìš©: pose_listëŠ” Noneìœ¼ë¡œ ì²˜ë¦¬
        pose_image_list = [None]*4

    # ì—¬ê¸°ì„œ num_samples_per_prompt = 1ë¡œ í•˜ë©´
    #   (4 pose Ã— 4 prompt Ã— 1 sample) = 16ì¥
    num_samples_per_prompt = 1

    inference_steps = 35
    faceid_scale = 1.0
    face_structure_scale = 0.5
    guidance_scale = 8.5

    # promptê°€ 4ê°œ, poseë„ 4ê°œ. 
    # "ê° í¬ì¦ˆì— ëŒ€í•´ ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±" í•˜ëŠ” ë°©ì‹.
    # ë”°ë¼ì„œ ë°”ê¹¥ ë£¨í”„: pose_image_list 4ê°œ
    #        ì•ˆìª½ ë£¨í”„: prompts 4ê°œ
    # ì´ 4Ã—4=16ì¥
    # ì‹œë“œëŠ” 4ê°œë¿ì´ë¯€ë¡œ, 4ê°œì˜ promptì— ë§ì¶° ë°˜ë³µí•´ì„œ ì‚¬ìš©
    seeds = [10, 10, 10, 10]

    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ â†’ PIL ë³€í™˜
    pil_images = []
    for file in pil_faceid_images:
        if isinstance(file, bytes):
            pil_image = Image.open(BytesIO(file)).convert("RGB")
        else:
            pil_image = file
        pil_images.append(pil_image)

    # faceid cond kwargs êµ¬ì„±
    cond_faceids = []
    for pil_faceid_image in pil_images:
        faceid_cond_kwargs = prepare_single_faceid_cond_kwargs(pil_faceid_image)
        if faceid_cond_kwargs is not None:
            cond_faceids.append(faceid_cond_kwargs)

    # Reset attention arguments
    attn_args.reset()
    attn_args.lora_scale = 1.0 if len(cond_faceids) == 1 else 0.0
    attn_args.multi_id_lora_scale = 1.0 if len(cond_faceids) > 1 else 0.0
    attn_args.faceid_scale = faceid_scale if len(cond_faceids) > 0 else 0.0
    attn_args.num_faceids = len(cond_faceids)
    print(attn_args)

    image_resolution = "512x768"
    h, w = map(int, image_resolution.split("x"))

    final_out = []

    # ë°”ê¹¥ ë£¨í”„(í¬ì¦ˆ 4ê°œ)
    for i, pose_path in enumerate(pose_image_list):
        if pose_path and os.path.exists(pose_path):
            controlnet_conditioning_image = Image.open(pose_path).convert("RGB")
            controlnet_conditioning_image = pose_processor(controlnet_conditioning_image)
            print("ControlNet ì ìš©:", pose_path)
            controlnet_scale = [0.8]  # ControlNet ì ìš© ê°•ë„
        else:
            print("ControlNet ë¯¸ì ìš©")
            controlnet_conditioning_image = None
            controlnet_scale = [0.0]

        # ì•ˆìª½ ë£¨í”„(í”„ë¡¬í”„íŠ¸ 4ê°œ)
        for j, (prompt, negative_prompt) in enumerate(zip(prompts, negative_prompts)):
            seed = seeds[j % len(seeds)]  # prompt ì¸ë±ìŠ¤ì— ë§ê²Œ seed ì„ íƒ

            # promptë¥¼ num_samples_per_prompt ë²ˆ ìƒì„±
            prompts_list = [prompt]*num_samples_per_prompt
            negative_prompts_list = [negative_prompt]*num_samples_per_prompt

            print("Prompt:", prompt)

            images = uniportrait_pipeline.generate(
                prompt=prompts_list,
                negative_prompt=negative_prompts_list,
                cond_faceids=cond_faceids,
                face_structure_scale=face_structure_scale,
                seed=seed if seed != -1 else None,
                guidance_scale=guidance_scale,
                num_inference_steps=inference_steps,
                image=[controlnet_conditioning_image] if controlnet_conditioning_image else None,
                controlnet_conditioning_scale=controlnet_scale,
            )
            final_out.extend(images)

    return final_out

def text_to_multi_id_generation_block():
    gr.Markdown("## ê°€ì¡± ì‚¬ì§„ ìƒì„±")
    gr.HTML(text_to_multi_id_description)
    gr.HTML(text_to_multi_id_tips)
    with gr.Row():
        with gr.Column(scale=2, min_width=100):
            pil_faceid_images = gr.File(
                file_count="multiple",
                file_types=["image"],
                type="binary",
                label="ì–¼êµ´ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            )
        with gr.Column(scale=1, min_width=100):
            run_button = gr.Button(value="ìƒì„±í•˜ê¸°")
    with gr.Row():
        result_gallery = gr.Gallery(
            label="ìƒì„±ëœ ì´ë¯¸ì§€",
            show_label=True,
            elem_id="gallery",
            columns=4,
            preview=True,
            format="png",
        )

    ips = [pil_faceid_images]
    run_button.click(fn=text_to_multi_id_generation_process, inputs=ips, outputs=[result_gallery])

if __name__ == "__main__":
    os.environ["no_proxy"] = "localhost,127.0.0.1,::1"

    title = r"""
            <div style="text-align: center;">
                <h1> ê°€ì¡± ì‚¬ì§„ ìƒì„± ë°ëª¨ </h1>
                <div style="display: flex; justify-content: center; align-items: center; text-align: center;">
                    <p>2024í•™ë…„ë„ 2í•™ê¸° ì‚°í•™ìº¡ìŠ¤í†¤ ë””ìì¸ 'ê¸°ìˆ  í•œ ìŠ¤í‘¼' íŒ€ì´ ì´ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>
                    <p>ì›ë³¸ ë…¼ë¬¸ì€ UniPortraitë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.</p>
                </div>
                </br>
            </div>
        """

    title_description = r"""
        ì´ ë°ëª¨ëŠ” ì—…ë¡œë“œí•œ ì–¼êµ´ ì´ë¯¸ì§€ ìˆ˜ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ì™€ ControlNet í¬ì¦ˆë¥¼ ìë™ ì ìš©í•©ë‹ˆë‹¤.<br>
        í´ë”ì—ì„œ 4ê°œì˜ í¬ì¦ˆ ì´ë¯¸ì§€ë¥¼ ë½‘ì•„, 4ê°œì˜ í”„ë¡¬í”„íŠ¸ ê°ê°ê³¼ ì¡°í•©í•˜ì—¬ ì´ 16ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.<br>
        """

    text_to_multi_id_description = r"""ğŸš€ğŸš€ğŸš€ë¹ ë¥¸ ì‹œì‘:<br>
        1. ì–¼êµ´ì´ ìˆëŠ” ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•œ í›„ <b>ìƒì„±í•˜ê¸°</b> ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”. ğŸ¤—<br>
        2. ì—…ë¡œë“œí•œ ì–¼êµ´ì´ í¬í•¨ëœ ê°€ì¡± ì‚¬ì§„ì„ ë§Œë“¤ì–´ ë“œë¦½ë‹ˆë‹¤.
        """

    text_to_multi_id_tips = r"""ğŸ’¡ğŸ’¡ğŸ’¡íŒ:<br>
        1. ë°°ê²½ì´ ê¹”ë”í•œ ì •ë©´ ì‚¬ì§„ì„ ì‚¬ìš©í•˜ëŠ”ê±¸ ì¶”ì²œ ë“œë¦½ë‹ˆë‹¤.
        2. ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ì €ì¥í•´ ì£¼ì„¸ìš”!
        """

    block = gr.Blocks(title="ê°€ì¡± ì‚¬ì§„ ìƒì„±").queue()
    with block:
        gr.HTML(title)
        gr.HTML(title_description)

        with gr.TabItem("í…ìŠ¤íŠ¸-ë‹¤ì¤‘-ì•„ì´ë”” ìƒì„±"):
            text_to_multi_id_generation_block()

    block.launch(server_name='0.0.0.0', share=True, server_port=port, allowed_paths=["/"])